{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNISTusingRNN/BiRNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarthakNarayan/DL-and-ML/blob/master/googlecolab/MNISTusingRNN_BiRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JwkPzVV77o9",
        "colab_type": "text"
      },
      "source": [
        "# Best Explanation of RNN\n",
        "![Best Explanation](https://i.stack.imgur.com/0Poch.png)<br/>\n",
        "Several LSTM cells form one LSTM layer. This is shown in the figure below. Since you are mostly dealing with data that is very extensive, it is not possible to incorporate everything in one piece into the model. Therefore, data is divided into small pieces as batches, which are processed one after the other until the batch containing the last part is read in. In the lower part of the figure you can see the input (dark grey) where the batches are read in one after the other from batch 1 to batch batch_size. The cells LSTM cell 1 to LSTM cell time_step above represent the described cells of the LSTM model (http://colah.github.io/posts/2015-08-Understanding-LSTMs/). The number of cells is equal to the number of fixed time steps. For example, if you take a text sequence with a total of 150 characters, you could divide it into 3 (batch_size) and have a sequence of length 50 per batch (number of time_steps and thus of LSTM cells). If you then encoded each character one-hot, each element (dark gray boxes of the input) would represent a vector that would have the length of the vocabulary (number of features). These vectors would flow into the neuronal networks (green elements in the cells) in the respective cells and would change their dimension to the length of the number of hidden units (number_units). So the input has the dimension (batch_size x time_step x features). The Long Time Memory (Cell State) and Short Time Memory (Hidden State) have the same dimensions (batch_size x number_units). The light gray blocks that arise from the cells have a different dimension because the transformations in the neural networks (green elements) took place with the help of the hidden units (batch_size x time_step x number_units). The output can be returned from any cell but mostly only the information from the last block (black border) is relevant (not in all problems) because it contains all information from the previous time steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt0i1f1O7hcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch import nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mvjAJ3hd0-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = torchvision.datasets.MNIST(root='data',\n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='data',\n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "length_of_train_set = len(train_dataset)\n",
        "fraction = 0.2\n",
        "length_of_validation_set = int(fraction*length_of_train_set)\n",
        "resulting_train_length = length_of_train_set - length_of_validation_set\n",
        "\n",
        "new_train_dataset , validation_dataset = torch.utils.data.random_split(train_dataset , [resulting_train_length,length_of_validation_set])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvX_Pwvye1vN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "no_hidden_units = 100\n",
        "sequence_length = 28\n",
        "# each row with 28 pixels\n",
        "input_size = 28\n",
        "# Since there are 10 classes\n",
        "output_size = 10\n",
        "num_layers = 2\n",
        "num_epochs = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hufr5ZFd5mv",
        "colab_type": "code",
        "outputId": "19e5eacb-6647-4409-b7db-b81cb0e443e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=new_train_dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "print(\"Lengh of trainset {}\".format(len(new_train_dataset)))\n",
        "print(\"Lengh of testset {}\".format(len(test_dataset)))\n",
        "print(\"Lengh of validationset {}\".format(len(validation_dataset)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lengh of trainset 48000\n",
            "Lengh of testset 10000\n",
            "Lengh of validationset 12000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-quLCwntfZ0C",
        "colab_type": "code",
        "outputId": "9e17333f-8083-4665-8070-f42660bc4639",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "class Lstm(nn.Module):\n",
        "    def __init__(self , input_size , no_hidden_units , output_size , num_layers , bidirectional = False):\n",
        "        super(Lstm , self).__init__()\n",
        "        \n",
        "        # since we will be using these values in the next function\n",
        "        self.bidirectional = bidirectional\n",
        "        self.no_hidden_units = no_hidden_units\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        if self.bidirectional == False:\n",
        "            # Here input_size is the number of features \n",
        "            # Can test the performance of RNNs and GRUs\n",
        "            self.lstm = nn.LSTM(input_size , self.no_hidden_units , \n",
        "                                num_layers , batch_first = True)\n",
        "            self.fc = nn.Linear(self.no_hidden_units , output_size)\n",
        "            \n",
        "        elif self.bidirectional == True:\n",
        "            self.lstm = nn.LSTM(input_size , self.no_hidden_units , \n",
        "                                num_layers , batch_first = True ,\n",
        "                                bidirectional = True)\n",
        "            self.fc = nn.Linear(self.no_hidden_units*2 , output_size)\n",
        "        \n",
        "    def forward(self , x):\n",
        "        # Would only have been hidden if RNN would have been used\n",
        "        # hidden = torch.zeros(self.num_layers ,x.size(0) ,self.no_hidden_units)\n",
        "        \n",
        "        if self.bidirectional == False: \n",
        "            h0 = torch.zeros(self.num_layers ,x.size(0) ,self.no_hidden_units).cuda()\n",
        "            c0 = torch.zeros(self.num_layers ,x.size(0) ,self.no_hidden_units).cuda()\n",
        "            \n",
        "        elif self.bidirectional == True:\n",
        "            h0 = torch.zeros(self.num_layers*2 ,x.size(0) ,self.no_hidden_units).cuda()\n",
        "            c0 = torch.zeros(self.num_layers*2 ,x.size(0) ,self.no_hidden_units).cuda()\n",
        "            \n",
        "        out , _ = self.lstm(x , (h0 , c0))\n",
        "        # out: tensor of shape (batch_size, seq_length, hidden_size*2) for bidirectional\n",
        "        # getting the last output from the sequence\n",
        "        out = out[:,-1,:]\n",
        "        output = self.fc(out)\n",
        "        return output\n",
        "    \n",
        "lstm = Lstm(input_size , no_hidden_units , output_size , num_layers , True)\n",
        "lstm.cuda()\n",
        "print(lstm)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lstm(\n",
            "  (lstm): LSTM(28, 100, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (fc): Linear(in_features=200, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_-JxV9ymM0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(lstm.parameters() , lr=1e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol_3Pg2wlPZ5",
        "colab_type": "code",
        "outputId": "f58b23f2-c434-450f-ea46-d74092b33f93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        }
      },
      "source": [
        "accuracy_max = 0\n",
        "for i in range(num_epochs):\n",
        "    \n",
        "    running_train_loss = 0\n",
        "    correct = 0\n",
        "    accuracy = 0\n",
        "    running_validation_loss = 0\n",
        "    # Very important to have train at the beginning\n",
        "    # because the program might break after training at evaluation \n",
        "    # Hence validation state will continue and there will be no way to go\n",
        "    # back to train state\n",
        "    lstm.train()\n",
        "    for images , labels in train_loader:\n",
        "        # making images and labels cuda compatible\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        images = images.reshape(-1 ,sequence_length , input_size)\n",
        "        # making the gradients zero\n",
        "        optimizer.zero_grad()\n",
        "        #forward pass\n",
        "        logits = lstm(images)\n",
        "        # calculating the loss\n",
        "        loss = criterion(logits , labels)\n",
        "        # backward propagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_train_loss += loss\n",
        "    \n",
        "    print(\"Training loss after epoch {}/{} is {} \".format(i+1 , num_epochs , running_train_loss/len(train_loader)))\n",
        "    \n",
        "    # putting the mode in evaluation mode\n",
        "    lstm.eval()\n",
        "    \n",
        "    # Since we dont want to compute gradients \n",
        "    with torch.no_grad():\n",
        "        for images_val , labels_val in validation_loader:\n",
        "            images_val = images_val.cuda()\n",
        "            labels_val = labels_val.cuda()\n",
        "            images_val = images_val.reshape(-1 ,sequence_length , input_size)\n",
        "            \n",
        "            prediction = lstm(images_val)\n",
        "            values , indices = torch.max(prediction , 1)\n",
        "            valid_loss = criterion(prediction , labels_val)\n",
        "            \n",
        "            running_validation_loss += valid_loss\n",
        "            for j in range(len(indices)):\n",
        "                if (indices[j] == labels_val[j]):\n",
        "                    correct += 1\n",
        "    \n",
        "    accuracy = (correct/len(validation_dataset))*100\n",
        "    running_validation_loss = running_validation_loss/len(validation_loader)\n",
        "    print(\"Validation loss and accuracy after epoch {}/{} is {} and {}\".format(i+1 , \n",
        "                                                                               num_epochs , \n",
        "                                                                               running_validation_loss,\n",
        "                                                                               accuracy))\n",
        "    if accuracy_max < accuracy:\n",
        "        accuracy_max = accuracy\n",
        "        print(\"Maximum validation accuracy of {} at epoch {}/{}\".format(accuracy_max,\n",
        "                                                                    i+1 , \n",
        "                                                                    num_epochs))\n",
        "        print(\"saving the model \\n\")\n",
        "        torch.save(lstm.state_dict(), '/content/TransferLearning.pth')\n",
        "    else:\n",
        "        print()\n",
        "\n",
        "print(\"\\n Training Over\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss after epoch 1/2 is 0.6576919555664062 \n",
            "Validation loss and accuracy after epoch 1/2 is 0.2457413524389267 and 92.525\n",
            "Maximum validation accuracy of 92.525 at epoch 1/2\n",
            "saving the model \n",
            "\n",
            "Training loss after epoch 2/2 is 0.19110603630542755 \n",
            "Validation loss and accuracy after epoch 2/2 is 0.13701337575912476 and 95.76666666666667\n",
            "Maximum validation accuracy of 95.76666666666667 at epoch 2/2\n",
            "saving the model \n",
            "\n",
            "\n",
            " Training Over\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNLvPVL2l8v-",
        "colab_type": "code",
        "outputId": "7b9256ad-fe24-4d3a-943e-27bc57c97d70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "# loading the weights of the best model for testing\n",
        "model_loaded = lstm\n",
        "model_loaded.cuda()\n",
        "model_loaded.load_state_dict(torch.load('/content/TransferLearning.pth')) \n",
        "model_loaded.eval()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Lstm(\n",
              "  (lstm): LSTM(28, 100, num_layers=2, batch_first=True, bidirectional=True)\n",
              "  (fc): Linear(in_features=200, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVZ043mkmAHT",
        "colab_type": "code",
        "outputId": "cfdddc77-9772-4ef3-dfd6-38d7996ec5bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# testing the model\n",
        "correct = 0\n",
        "accuracy = 0\n",
        "with torch.no_grad():\n",
        "        for images_test , labels_test in test_loader:\n",
        "            images_test = images_test.cuda()\n",
        "            labels_test = labels_test.cuda()\n",
        "            images_test = images_test.reshape(-1 ,sequence_length , input_size)\n",
        "            prediction = model_loaded(images_test)\n",
        "            values , indices = torch.max(prediction , 1)\n",
        "            test_loss = criterion(prediction , labels_test)\n",
        "            \n",
        "            for j in range(len(indices)):\n",
        "                if (indices[j] == labels_test[j]):\n",
        "                    correct += 1\n",
        "    \n",
        "accuracy = (correct/len(test_dataset))*100\n",
        "print(\"Test accuracy is {}\".format(accuracy))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy is 95.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPZCJNWQ5m2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}