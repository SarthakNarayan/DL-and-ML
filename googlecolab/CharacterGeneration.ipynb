{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CharacterGeneration.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarthakNarayan/DL-and-ML/blob/master/googlecolab/CharacterGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgoZN2Hdlckc",
        "colab_type": "text"
      },
      "source": [
        "##Using colors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPrMH6eSla2V",
        "colab_type": "code",
        "outputId": "ffa35704-57e2-4c5d-fb13-4b23d097f82c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "class color:\n",
        "    PURPLE = '\\033[95m'\n",
        "    CYAN = '\\033[96m'\n",
        "    DARKCYAN = '\\033[36m'\n",
        "    BLUE = '\\033[94m'\n",
        "    GREEN = '\\033[92m'\n",
        "    YELLOW = '\\033[93m'\n",
        "    RED = '\\033[91m'\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'\n",
        "    END = '\\033[0m'\n",
        "\n",
        "print (color.GREEN + 'Hello World !' )\n",
        "print(\"happens if u dont end it\")\n",
        "print(color.END)\n",
        "print(\"will not happen now\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[92mHello World !\n",
            "happens if u dont end it\n",
            "\u001b[0m\n",
            "will not happen now\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8QNxvMblhBE",
        "colab_type": "text"
      },
      "source": [
        "##Imports Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es-JaUTT1Ele",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import torchvision\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai7VNGKM2Lgq",
        "colab_type": "text"
      },
      "source": [
        "##Loading the data\n",
        "Link to the repo containing text file<br/>\n",
        "<https://github.com/udacity/deep-learning><br/>\n",
        "Go to tensorboard and there you will find anna.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPr3Thmk2RLN",
        "colab_type": "code",
        "outputId": "ea497611-67cd-4561-a20c-4909a79c8ef8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "with open(\"/content/deep-learning/tensorboard/anna.txt\" , 'r') as f:\n",
        "    text = f.read()\n",
        "print(\"Total Number of characters in the text {}\".format(len(text)))\n",
        "# characters_to_be_read = 500001\n",
        "# text = text[:characters_to_be_read]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Number of characters in the text 1985223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iicz4KIZ2_YU",
        "colab_type": "text"
      },
      "source": [
        "##Tokenization\n",
        "We want to map every character to a unique index since our network can only learn from numerical data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I5R3OFn3SYB",
        "colab_type": "code",
        "collapsed": true,
        "outputId": "1c65d5e9-04cc-4fbd-8ed6-5026f2cd7ce5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "# Separates every unique character present in the input file \n",
        "# chars is also known as vocabulary\n",
        "chars = tuple(set(text))\n",
        "print(chars)\n",
        "print(\"No of unique characters {} \\n\".format(len(chars)))\n",
        "\n",
        "# Gives each of the characters a unique number starting from 0 by creating a dictionary\n",
        "map_int_to_char = dict(enumerate(chars))\n",
        "print(map_int_to_char)\n",
        "\n",
        "# Reverse map numbers to characters\n",
        "map_char_to_int = {}\n",
        "for ii , char in map_int_to_char.items():\n",
        "    map_char_to_int[char]=ii\n",
        "    \n",
        "print(map_char_to_int , \"\\n\")\n",
        "\n",
        "# Since we have characters as numbers we need to encode the input text\n",
        "encoded = []\n",
        "for ch in text:\n",
        "    encoded.append(map_char_to_int[ch])\n",
        "\n",
        "print(\"Encoded Text\")\n",
        "print(encoded[:20])\n",
        "print(len(encoded))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('`', 'Y', '1', 'v', 'U', '-', 'B', 'y', 'M', '2', 'o', ';', 'q', ' ', 'b', 'O', \"'\", 'l', 'g', '!', 'i', 'G', 'V', 'I', '7', '/', 'Z', '$', '5', 'P', 'K', '@', ':', 's', 'c', 'T', 'L', '%', 'C', 'h', 't', 'e', '\\n', 'f', 'A', 'N', 'p', 'E', ',', 'H', 'j', '(', 'd', 'R', '?', 'm', 'u', 'n', '4', 'X', 'S', 'D', ')', 'w', '.', '&', 'x', '\"', '6', 'r', '3', '8', '0', 'F', '_', 'W', '9', 'a', 'z', 'Q', 'k', 'J', '*')\n",
            "No of unique characters 83 \n",
            "\n",
            "{0: '`', 1: 'Y', 2: '1', 3: 'v', 4: 'U', 5: '-', 6: 'B', 7: 'y', 8: 'M', 9: '2', 10: 'o', 11: ';', 12: 'q', 13: ' ', 14: 'b', 15: 'O', 16: \"'\", 17: 'l', 18: 'g', 19: '!', 20: 'i', 21: 'G', 22: 'V', 23: 'I', 24: '7', 25: '/', 26: 'Z', 27: '$', 28: '5', 29: 'P', 30: 'K', 31: '@', 32: ':', 33: 's', 34: 'c', 35: 'T', 36: 'L', 37: '%', 38: 'C', 39: 'h', 40: 't', 41: 'e', 42: '\\n', 43: 'f', 44: 'A', 45: 'N', 46: 'p', 47: 'E', 48: ',', 49: 'H', 50: 'j', 51: '(', 52: 'd', 53: 'R', 54: '?', 55: 'm', 56: 'u', 57: 'n', 58: '4', 59: 'X', 60: 'S', 61: 'D', 62: ')', 63: 'w', 64: '.', 65: '&', 66: 'x', 67: '\"', 68: '6', 69: 'r', 70: '3', 71: '8', 72: '0', 73: 'F', 74: '_', 75: 'W', 76: '9', 77: 'a', 78: 'z', 79: 'Q', 80: 'k', 81: 'J', 82: '*'}\n",
            "{'`': 0, 'Y': 1, '1': 2, 'v': 3, 'U': 4, '-': 5, 'B': 6, 'y': 7, 'M': 8, '2': 9, 'o': 10, ';': 11, 'q': 12, ' ': 13, 'b': 14, 'O': 15, \"'\": 16, 'l': 17, 'g': 18, '!': 19, 'i': 20, 'G': 21, 'V': 22, 'I': 23, '7': 24, '/': 25, 'Z': 26, '$': 27, '5': 28, 'P': 29, 'K': 30, '@': 31, ':': 32, 's': 33, 'c': 34, 'T': 35, 'L': 36, '%': 37, 'C': 38, 'h': 39, 't': 40, 'e': 41, '\\n': 42, 'f': 43, 'A': 44, 'N': 45, 'p': 46, 'E': 47, ',': 48, 'H': 49, 'j': 50, '(': 51, 'd': 52, 'R': 53, '?': 54, 'm': 55, 'u': 56, 'n': 57, '4': 58, 'X': 59, 'S': 60, 'D': 61, ')': 62, 'w': 63, '.': 64, '&': 65, 'x': 66, '\"': 67, '6': 68, 'r': 69, '3': 70, '8': 71, '0': 72, 'F': 73, '_': 74, 'W': 75, '9': 76, 'a': 77, 'z': 78, 'Q': 79, 'k': 80, 'J': 81, '*': 82} \n",
            "\n",
            "Encoded Text\n",
            "[38, 39, 77, 46, 40, 41, 69, 13, 2, 42, 42, 42, 49, 77, 46, 46, 7, 13, 43, 77]\n",
            "1985223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmUjyRH8ljx8",
        "colab_type": "text"
      },
      "source": [
        "##Defining the variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gbVjTevOaRE",
        "colab_type": "code",
        "outputId": "7b7eb9e8-a08c-4e71-87d2-ffcec9b6d7a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Length of the sequence to be sampled from the text data\n",
        "length_of_sequence = 1000\n",
        "# no of batches required\n",
        "batch_size = 10\n",
        "# so step size will be length_of_sequence/batch_size\n",
        "\n",
        "no_hidden_layer = 512\n",
        "input_size = len(chars)\n",
        "output_size = len(chars)\n",
        "num_layers = 2\n",
        "num_epochs = 20\n",
        "print(input_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96kRK_rpCk-x",
        "colab_type": "text"
      },
      "source": [
        "##One hot encoding\n",
        "Now we need to convert our encoded array into one hot representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XQegxQdCs5T",
        "colab_type": "code",
        "outputId": "0f4b9091-0a17-4688-cdea-06bfc8ea3ab8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "# The comments are for understanding\n",
        "# uncomment them to see how it happens\n",
        "def one_hot(encoded_array , onehot_length):\n",
        "    # for [[1,2,3]] shape will give (1,3)\n",
        "    # size will give number of elements present\n",
        "    one_hot = np.zeros((encoded_array.size , onehot_length) , dtype = np.float32)\n",
        "#     print(one_hot.shape)\n",
        "    \n",
        "    # flatten the array\n",
        "#     print(encoded_array.flatten())\n",
        "#     print(np.arange(encoded_array.size))\n",
        "    one_hot[np.arange(encoded_array.size), encoded_array.flatten()] = 1.0\n",
        "#     print(one_hot)\n",
        "    \n",
        "    one_hot = one_hot.reshape((*encoded_array.shape, onehot_length))\n",
        "    return one_hot\n",
        "\n",
        "one_hot_encoded = one_hot(np.array([[1,2,3,4]]),6)\n",
        "print(one_hot_encoded)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yl89LNrV7p6",
        "colab_type": "text"
      },
      "source": [
        "##Creating Mini batches for training\n",
        "we want our batches to be multiple sequences of some desired number of sequence steps.<br/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6JPFuobmI_H",
        "colab_type": "code",
        "outputId": "745ad3a4-005e-4d9f-f172-983f796ccb4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      },
      "source": [
        "def batcher(arr , batch_size , job='train'):\n",
        "    length_of_array = len(arr)\n",
        "    if job=='train':\n",
        "        number_bathes = batch_size\n",
        "    elif job=='generate':\n",
        "        number_bathes = 1\n",
        "        \n",
        "    elements_per_batch = len(arr)//number_bathes\n",
        "    total_elements = elements_per_batch*number_bathes\n",
        "    arr = arr[:total_elements]\n",
        "    arr = arr.reshape(number_bathes , -1)\n",
        "    return arr\n",
        "\n",
        "x = encoded[:100]\n",
        "batched = batcher(np.array(x) , batch_size , 'train')\n",
        "print(batched)\n",
        "print(batched.shape)\n",
        "print(\"\\n Final shape of the input\")\n",
        "one_hot_encoded = one_hot(batched,len(chars))\n",
        "print(one_hot_encoded.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[38 39 77 46 40 41 69 13  2 42]\n",
            " [42 42 49 77 46 46  7 13 43 77]\n",
            " [55 20 17 20 41 33 13 77 69 41]\n",
            " [13 77 17 17 13 77 17 20 80 41]\n",
            " [11 13 41  3 41 69  7 13 56 57]\n",
            " [39 77 46 46  7 13 43 77 55 20]\n",
            " [17  7 13 20 33 13 56 57 39 77]\n",
            " [46 46  7 13 20 57 13 20 40 33]\n",
            " [13 10 63 57 42 63 77  7 64 42]\n",
            " [42 47  3 41 69  7 40 39 20 57]]\n",
            "(10, 10)\n",
            "\n",
            " Final shape of the input\n",
            "(10, 10, 83)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysIf9DObiCZN",
        "colab_type": "text"
      },
      "source": [
        "##Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MASst02UfZaE",
        "colab_type": "code",
        "outputId": "d1d16213-9f29-4687-8670-42e29f61393d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "class CharacterLstm(nn.Module):\n",
        "    def __init__(self , no_hidden_layer , num_layers):\n",
        "        super(CharacterLstm , self).__init__()\n",
        "        self.no_hidden_layer = no_hidden_layer\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size , self.no_hidden_layer ,\n",
        "                            self.num_layers , batch_first = True)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(self.no_hidden_layer , output_size)\n",
        "    \n",
        "    def forward(self ,x , c0 , h0):\n",
        "        lstm_out , (c , h) = self.lstm(x , (c0,h0))\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "        lstm_out = lstm_out.contiguous()\n",
        "        out = lstm_out.view(-1,self.no_hidden_layer)\n",
        "        out = self.fc(out)\n",
        "        return out , c , h\n",
        "    \n",
        "charlstm = CharacterLstm(no_hidden_layer , num_layers)\n",
        "charlstm.cuda()\n",
        "print(charlstm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharacterLstm(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True)\n",
            "  (dropout): Dropout(p=0.5)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4MhB5o2iEOT",
        "colab_type": "text"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhCelkCdpTs-",
        "colab_type": "code",
        "outputId": "9206a9fd-e083-4420-85e2-a7c8caf98de9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        }
      },
      "source": [
        "optimizer = optim.Adam(charlstm.parameters() , lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "start_time = time.time()\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    h0 = torch.zeros(num_layers , batch_size , no_hidden_layer).cuda()\n",
        "    c0 = torch.zeros(num_layers , batch_size , no_hidden_layer).cuda()\n",
        "    running_loss = 0\n",
        "    charlstm.train()\n",
        "    \n",
        "    for counter,step in enumerate(range(0,len(encoded),length_of_sequence),1):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        x = encoded[step:step+length_of_sequence]\n",
        "        y = encoded[step+1:step+length_of_sequence+1]\n",
        "        \n",
        "        if(len(y)<length_of_sequence):\n",
        "            print(\"Breaking\")\n",
        "            counter = counter-1\n",
        "            break\n",
        "        \n",
        "        batched_x = batcher(np.array(x) , batch_size)\n",
        "        batched_y = batcher(np.array(y) , batch_size)\n",
        "        one_hot_x = one_hot(batched_x,len(chars))\n",
        "        \n",
        "        one_hot_x = torch.from_numpy(one_hot_x).cuda()\n",
        "        batched_y = torch.from_numpy(batched_y).cuda()\n",
        "        \n",
        "        pred , c , h = charlstm(one_hot_x , c0 ,h0)\n",
        "        c = c.data\n",
        "        h = h.data\n",
        "        batched_y = batched_y.reshape(pred.shape[0]).long()\n",
        "        \n",
        "        loss = criterion(pred , batched_y)\n",
        "        loss.backward()\n",
        "        \n",
        "#         prevents exploding gradients problem in rnns and lstms\n",
        "#         nn.utils.clip_grad_norm_(charlstm.parameters(), 5)\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss\n",
        "        \n",
        "        if counter%2000==0:\n",
        "            printing_loss = running_loss/counter\n",
        "            print(\"Loss after {} steps in epoch {} is {}\".format(counter,i+1,printing_loss))\n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "    running_loss = running_loss/counter\n",
        "    print(color.RED + \"Loss after epoch {} is {}\".format(i+1,running_loss) + color.END)\n",
        "    \n",
        "end_time = time.time()\n",
        "print(\"Training Complete\")\n",
        "total_time = end_time - start_time\n",
        "print(\"Total time taken {}\".format(total_time))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Breaking\n",
            "\u001b[91mLoss after epoch 1 is 1.9476960897445679\u001b[0m\n",
            "Breaking\n",
            "\u001b[91mLoss after epoch 2 is 1.4227629899978638\u001b[0m\n",
            "Breaking\n",
            "\u001b[91mLoss after epoch 3 is 1.3010947704315186\u001b[0m\n",
            "Breaking\n",
            "\u001b[91mLoss after epoch 4 is 1.2352197170257568\u001b[0m\n",
            "Breaking\n",
            "\u001b[91mLoss after epoch 5 is 1.1909878253936768\u001b[0m\n",
            "Breaking\n",
            "\u001b[91mLoss after epoch 6 is 1.1578924655914307\u001b[0m\n",
            "Breaking\n",
            "\u001b[91mLoss after epoch 7 is 1.129645824432373\u001b[0m\n",
            "Breaking\n",
            "\u001b[91mLoss after epoch 8 is 1.1064649820327759\u001b[0m\n",
            "Breaking\n",
            "\u001b[91mLoss after epoch 9 is 1.0856143236160278\u001b[0m\n",
            "Breaking\n",
            "\u001b[91mLoss after epoch 10 is 1.067736029624939\u001b[0m\n",
            "Breaking\n",
            "\u001b[91mLoss after epoch 11 is 1.051243782043457\u001b[0m\n",
            "Breaking\n",
            "\u001b[91mLoss after epoch 12 is 1.0372923612594604\u001b[0m\n",
            "Breaking\n",
            "\u001b[91mLoss after epoch 13 is 1.024314045906067\u001b[0m\n",
            "Breaking\n",
            "\u001b[91mLoss after epoch 14 is 1.0127302408218384\u001b[0m\n",
            "Breaking\n",
            "\u001b[91mLoss after epoch 15 is 1.0028879642486572\u001b[0m\n",
            "Breaking\n",
            "\u001b[91mLoss after epoch 16 is 0.9923209547996521\u001b[0m\n",
            "Breaking\n",
            "\u001b[91mLoss after epoch 17 is 0.9837216734886169\u001b[0m\n",
            "Breaking\n",
            "\u001b[91mLoss after epoch 18 is 0.9756173491477966\u001b[0m\n",
            "Breaking\n",
            "\u001b[91mLoss after epoch 19 is 0.9668247103691101\u001b[0m\n",
            "Breaking\n",
            "\u001b[91mLoss after epoch 20 is 0.9602932929992676\u001b[0m\n",
            "Training Complete\n",
            "Total time taken 802.7966017723083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUcLoGQYwyJn",
        "colab_type": "text"
      },
      "source": [
        "##Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMDiWGkm-74l",
        "colab_type": "code",
        "outputId": "f4802556-d601-4bcb-ef6b-fa07e650e9a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "charlstm.eval()\n",
        "def text_generation(input_sequence , length_to_generate = 100):\n",
        "    \n",
        "    h0 = torch.zeros(num_layers , 1 , no_hidden_layer).cuda()\n",
        "    c0 = torch.zeros(num_layers , 1 , no_hidden_layer).cuda()\n",
        "    x = input_sequence\n",
        "    sentence = []\n",
        "    \n",
        "    for i in range(length_to_generate):\n",
        "        x = [map_char_to_int[x]]\n",
        "        batched_x = batcher(np.array(x) , 1 , 'generate')\n",
        "        one_hot_x = one_hot(batched_x,len(chars))\n",
        "        one_hot_x = torch.from_numpy(one_hot_x).cuda()\n",
        "        pred , c , h = charlstm(one_hot_x , c0 ,h0)\n",
        "        c = c.data\n",
        "        h = h.data\n",
        "        p = F.softmax(pred, dim=1).data.cpu()\n",
        "        \n",
        "        top_ch = np.arange(len(chars))\n",
        "        top_k = 5\n",
        "        p, top_ch = p.topk(top_k)\n",
        "        top_ch = top_ch.numpy().squeeze()\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        letter = map_int_to_char[char]\n",
        "\n",
        "#         _ , letter = torch.max(p , dim=1)\n",
        "#         letter = letter.item()\n",
        "#         letter = map_int_to_char[letter]\n",
        "        sentence.append(letter)\n",
        "        x = letter\n",
        "    \n",
        "    return ''.join(sentence)\n",
        "\n",
        "sentence = text_generation('b')\n",
        "print(sentence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "e the the the the the the the the the the the the the the the the the the the the the the the the th\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}