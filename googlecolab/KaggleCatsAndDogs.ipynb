{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KaggleCatsAndDogs.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarthakNarayan/DL-and-ML/blob/master/googlecolab/KaggleCatsAndDogs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iocNOVKdws4Z",
        "colab_type": "text"
      },
      "source": [
        "#Mouting Google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzEmBERRwrtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mounting google drive for saving the weights\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM0g1tIAgr3b",
        "colab_type": "text"
      },
      "source": [
        "#Getting the data from kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M_ijoME1yXF",
        "colab_type": "code",
        "outputId": "8e53fbd7-2533-42d5-a2cc-217ea2fd90db",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "# importing the json file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3e506009-af6d-4959-82df-dc01bb591d48\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-3e506009-af6d-4959-82df-dc01bb591d48\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ze91kq6OhjVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# commands for loading kaggle dataset\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "418Db4-Bg8wm",
        "colab_type": "code",
        "outputId": "677a1afe-4344-483b-dbad-f41cca24c030",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# loading the bees and ants dataset from kaggle\n",
        "!kaggle competitions download -c dogs-vs-cats"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading sampleSubmission.csv to /content\n",
            "\r  0% 0.00/86.8k [00:00<?, ?B/s]\n",
            "\r100% 86.8k/86.8k [00:00<00:00, 30.3MB/s]\n",
            "Downloading test1.zip to /content\n",
            " 98% 266M/271M [00:01<00:00, 211MB/s]\n",
            "100% 271M/271M [00:01<00:00, 206MB/s]\n",
            "Downloading train.zip to /content\n",
            " 99% 537M/543M [00:04<00:00, 153MB/s]\n",
            "100% 543M/543M [00:04<00:00, 127MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hnh-LQumhwht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unzipping the data\n",
        "!unzip \"/content/test1.zip\"\n",
        "!unzip \"/content/train.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41-l-hj4iDdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data has to be arranged in folders for training and test data to be used by pytorch\n",
        "!mkdir -p /content/trainArranged/cats\n",
        "!mkdir -p /content/trainArranged/dogs\n",
        "!mkdir -p /content/testArranged/cats\n",
        "!mkdir -p /content/testArranged/dogs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHvvoxYFNa1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "path_from_train = \"/content/train\"\n",
        "path_to_train = \"/content/trainArranged\"\n",
        "\n",
        "train_contents = os.listdir(path_from_train)\n",
        "test_contents = os.listdir(path_from_test)\n",
        "\n",
        "for files in train_contents:\n",
        "    if(files[0] == 'c'): \n",
        "        os.rename(path_from_train+\"/\"+files , \"/content/trainArranged/cats/\"+files)\n",
        "    else:\n",
        "        os.rename(path_from_train+\"/\"+files , \"/content/trainArranged/dogs/\"+files)\n",
        "\n",
        "# cats is label 0 and dogs is label 1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LEjhWm3igP0",
        "colab_type": "text"
      },
      "source": [
        "#ETL Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKchQWSWijT0",
        "colab_type": "code",
        "outputId": "d5adfd42-e94a-49ed-f235-cbe1f252e00a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Loading the train and test data\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "# data transforms\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n",
        "\n",
        "trainset = torchvision.datasets.ImageFolder(root='/content/trainArranged', \n",
        "                                            transform=data_transforms['train'],\n",
        "                                            )\n",
        "\n",
        "print(\"No of images in training set {}\".format(len(trainset)))\n",
        "# since in kaggle competitions test sets are not labelled we will split our train set from \n",
        "# training set"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No of images in training set 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-08-Jg-QkN2Q",
        "colab_type": "code",
        "outputId": "3dd5ea28-4890-444d-c415-02a89d7b1854",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# creating a validation and training set\n",
        "len_validationset = int(0.1*len(trainset))\n",
        "len_testset = int(0.1*len(trainset))\n",
        "print(\"The length of validation and test set is {}\".format(len_validationset))\n",
        "\n",
        "new_length_trainset = len(trainset) - len_validationset - len_testset\n",
        "trainset , validationset , testset = torch.utils.data.random_split(trainset , \n",
        "                                                                   [new_length_trainset,\n",
        "                                                                     len_validationset , \n",
        "                                                                    len_testset])\n",
        "\n",
        "# creating the data loaders\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=100,shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=100,shuffle=True)\n",
        "validation_loader = torch.utils.data.DataLoader(validationset, batch_size=100,shuffle=True)\n",
        "\n",
        "print(\"No of batches in train loader {}\".format(len(train_loader)))\n",
        "print(\"No of batches in test loader {}\".format(len(test_loader)))\n",
        "print(\"No of batches in validation loader {}\".format(len(validation_loader)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The length of validation and test set is 2000\n",
            "No of batches in train loader 160\n",
            "No of batches in test loader 20\n",
            "No of batches in validation loader 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftpJQAQkl1Q8",
        "colab_type": "text"
      },
      "source": [
        "#Loading/Making the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh3rPDcvlxcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Since we are performing transfer learning we will load a model\n",
        "from torchvision import models\n",
        "# loading a resnet model with 18 layers\n",
        "transfer_model = models.resnet18(pretrained=True)\n",
        "print(transfer_model)\n",
        "# on visualizing the layer we observe we have to replace the fc layer since we have only 2 classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp4a6Nbamk67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "class ReplacementLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ReplacementLayer , self).__init__()\n",
        "        self.fc1 = nn.Linear(512 , 128)\n",
        "        self.fc2 = nn.Linear(128 , 64)\n",
        "        self.output = nn.Linear(64 , 2)\n",
        "        \n",
        "    def forward(self , x):\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "end_layer = ReplacementLayer()\n",
        "print(\"The last layers are :\")\n",
        "print(end_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9if_bBHoJhm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# attaching our end_layer inplace of resnet fc layerer\n",
        "transfer_model.fc = end_layer\n",
        "# converting the model to make it cuda compatible\n",
        "transfer_model = transfer_model.cuda()\n",
        "print(transfer_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pztbMOlVowGB",
        "colab_type": "text"
      },
      "source": [
        "#Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00liM8f5op4f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "392f250b-eb72-43e1-ddee-37317977de2b"
      },
      "source": [
        "import torch.optim as optim\n",
        "# loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# defining the optimizer\n",
        "# Since we want other layers to remain frozen and only update the weights of our fc layer\n",
        "optimizer = optim.Adam(transfer_model.fc.parameters() , lr=1e-3)\n",
        "\n",
        "num_epochs = 5\n",
        "accuracy_max = 0\n",
        "for i in range(num_epochs):\n",
        "    \n",
        "    running_train_loss = 0\n",
        "    correct = 0\n",
        "    accuracy = 0\n",
        "    running_validation_loss = 0\n",
        "    \n",
        "    for images , labels in train_loader:\n",
        "        # making images and labels cuda compatible\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        # making the gradients zero\n",
        "        optimizer.zero_grad()\n",
        "        #forward pass\n",
        "        logits = transfer_model(images)\n",
        "        # calculating the loss\n",
        "        loss = criterion(logits , labels)\n",
        "        # backward propagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_train_loss += loss\n",
        "    \n",
        "    print(\"Training loss after epoch {}/{} is {} \".format(i+1 , num_epochs , running_train_loss/len(train_loader)))\n",
        "    \n",
        "    # putting the mode in evaluation mode\n",
        "    transfer_model.eval()\n",
        "    \n",
        "    # Since we dont want to compute gradients \n",
        "    with torch.no_grad():\n",
        "        for images_val , labels_val in validation_loader:\n",
        "            images_val = images_val.cuda()\n",
        "            labels_val = labels_val.cuda()\n",
        "            \n",
        "            prediction = transfer_model(images_val)\n",
        "            values , indices = torch.max(prediction , 1)\n",
        "            valid_loss = criterion(prediction , labels_val)\n",
        "            \n",
        "            running_validation_loss += valid_loss\n",
        "            for j in range(len(indices)):\n",
        "                if (indices[j] == labels_val[j]):\n",
        "                    correct += 1\n",
        "    \n",
        "    accuracy = (correct/len(validationset))*100\n",
        "    running_validation_loss = running_validation_loss/len(validation_loader)\n",
        "    print(\"Validation loss and accuracy after epoch {}/{} is {} and {}\".format(i+1 , \n",
        "                                                                               num_epochs , \n",
        "                                                                               running_validation_loss,\n",
        "                                                                               accuracy))\n",
        "    transfer_model.train()\n",
        "    if accuracy_max < accuracy:\n",
        "        accuracy_max = accuracy\n",
        "        print(\"Maximum validation accuracy of {} at epoch {}/{}\".format(accuracy_max,\n",
        "                                                                    i+1 , \n",
        "                                                                    num_epochs))\n",
        "        print(\"saving the model \\n\")\n",
        "        torch.save(transfer_model.state_dict(), '/content/gdrive/My Drive/Colab Notebooks/TransferLearning.pth')\n",
        "    else:\n",
        "        print()\n",
        "\n",
        "print(\"\\n Training Over\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss after epoch 1/5 is 0.1738729029893875 \n",
            "Validation loss and accuracy after epoch 1/5 is 0.13371403515338898 and 94.65\n",
            "Maximum validation accuracy of 94.65 at epoch 1/5\n",
            "saving the model \n",
            "\n",
            "Training loss after epoch 2/5 is 0.1328398436307907 \n",
            "Validation loss and accuracy after epoch 2/5 is 0.1652081459760666 and 92.75\n",
            "\n",
            "Training loss after epoch 3/5 is 0.14476324617862701 \n",
            "Validation loss and accuracy after epoch 3/5 is 0.1498027890920639 and 93.35\n",
            "\n",
            "Training loss after epoch 4/5 is 0.12865065038204193 \n",
            "Validation loss and accuracy after epoch 4/5 is 0.1231127604842186 and 94.95\n",
            "Maximum validation accuracy of 94.95 at epoch 4/5\n",
            "saving the model \n",
            "\n",
            "Training loss after epoch 5/5 is 0.13060401380062103 \n",
            "Validation loss and accuracy after epoch 5/5 is 0.1254262775182724 and 94.6\n",
            "\n",
            "\n",
            " Training Over\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T64fb6QjxZ9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading the weights of the best model for testing\n",
        "model_loaded = transfer_model\n",
        "model_loaded.cuda()\n",
        "model_loaded.load_state_dict(torch.load('/content/gdrive/My Drive/Colab Notebooks/TransferLearning.pth')) \n",
        "model_loaded.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syTuBZ4I1-mr",
        "colab_type": "code",
        "outputId": "06dcd67b-8fff-4b5b-9d3f-7747b35615d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# testing the model\n",
        "correct = 0\n",
        "accuracy = 0\n",
        "with torch.no_grad():\n",
        "        for images_test , labels_test in test_loader:\n",
        "            images_test = images_test.cuda()\n",
        "            labels_test = labels_test.cuda()\n",
        "            prediction = model_loaded(images_test)\n",
        "            values , indices = torch.max(prediction , 1)\n",
        "            test_loss = criterion(prediction , labels_test)\n",
        "            \n",
        "            for j in range(len(indices)):\n",
        "                if (indices[j] == labels_test[j]):\n",
        "                    correct += 1\n",
        "    \n",
        "accuracy = (correct/len(testset))*100\n",
        "print(\"Test accuracy is {}\".format(accuracy))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy is 94.95\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}